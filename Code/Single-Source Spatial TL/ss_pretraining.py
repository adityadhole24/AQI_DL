# -*- coding: utf-8 -*-
"""SS_Pretraining.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1recVEkn7EQXNppR2b1kmBzX8Fr64df-b
"""

from singlestep import *

from math import sqrt
import pandas as pd
import numpy as np
import pickle
import os
from sklearn.metrics import mean_squared_error
from keras.layers import Dense,LSTM,Bidirectional,BatchNormalization,Dropout,Conv1D,GRU,MaxPooling1D
from keras.models import Sequential,Model
from keras.layers.merge import concatenate
from keras.callbacks import History, ModelCheckpoint
from keras import optimizers
from keras.engine.input_layer import Input
from matplotlib import pyplot
from sklearn.preprocessing import MinMaxScaler

def n_split(data):  #CORRECT
    train,test=data[0:365*24*1+24*30*4],data[365*24*1+24*30*4:365*24*1+24*30*6]
    train=np.split(train,len(train)/24)
    test=np.split(test,len(test)/24)
    return np.array(train),np.array(test)

def main():

  data_fp=[]
  data_fp.insert(0,'/content/Aotizhongxinpreproc.csv')
  data_fp.insert(1,'/content/Dongsipreproc.csv')
  data_fp.insert(2,'/content/Guanyuanpreproc.csv')
  data_fp.insert(3,'/content/Guchengpreproc.csv')
  data_fp.insert(4,'/content/Huairoupreproc.csv')
  data_fp.insert(5,'/content/Nongzhanguanpreproc.csv')
  data_fp.insert(6,'/content/Shunyipreproc.csv')
  data_fp.insert(7,'/content/Tiantanpreproc.csv')
  data_fp.insert(8,'/content/Wanliupreproc.csv')
  data_fp.insert(9,'/content/Wanshouxigongpreproc.csv')
  
  i=0
  for city in data_fp:
    print(city)
    data=pd.read_csv(city,header=0,parse_dates=True,index_col=['No'])
    n_input=6
    min=data['PM2.5'].min()
    max=data['PM2.5'].max() 
    scaler = MinMaxScaler(feature_range=(0, 1))
    data = scaler.fit_transform(data)
      
    train,test=n_split(data)
    print(train.shape)
    print(test.shape)
    train_x,train_y=to_supervised(train,n_input,1)
    n_timesteps,n_features,n_outputs=train_x.shape[1],train_x.shape[2],train_y.shape[1]
    print(train_x.shape)
    print(train_y.shape)

    model=CNN_GRU(n_timesteps)
    history = fit_model(train_x,train_y,model,0,0)
    scores,final_score= model_eval(train,test,n_input,model,24,0)
    scores=[t*(max-min)+min for t in scores]
    print(scores)
    print(final_score*(max-min)+min)

    filename = 'fm'+str(i)+'.sav'
    pickle.dump(model, open(filename, 'wb'))
    os.remove('/content/best_weights0.hdf5')
    i=i+1

if __name__=='__main__':
  main()