# -*- coding: utf-8 -*-
"""singlestep.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1APNI67tBgR5B1Htbbiq4Fjx8G7XHvZww
"""

def func():
  print("Hola!\nif you hear me")

from math import sqrt
import pandas as pd
import numpy as np
import pickle
import os
from sklearn.metrics import mean_squared_error
from keras.layers import Dense,LSTM,Bidirectional,BatchNormalization,Dropout,Conv1D,GRU,MaxPooling1D
from keras.models import Sequential,Model
from keras.layers.merge import concatenate
from keras.callbacks import History, ModelCheckpoint
from keras import optimizers
from keras.engine.input_layer import Input
from matplotlib import pyplot
from sklearn.preprocessing import MinMaxScaler

def fill_miss(values):
  one_day=24
  for i in range(values.shape[0]):
    for j in range(values.shape[1]):
      if np.isnan(values[i,j]):
        data.fillna(method='ffill',inplace=True)

def split(data):  #CORRECT
    train,test=data[0:365*24*3],data[(365*24*3):(365*24*4)]
    train=np.split(train,len(train)/24)
    test=np.split(test,len(test)/24)
    return np.array(train),np.array(test)

def new_split(data):  #CORRECT
    train,test=data[0:365*24*1],data[(365*24*1):(365*24*2)]
    train=np.split(train,len(train)/24)
    test=np.split(test,len(test)/24)
    return np.array(train),np.array(test)

def eval_forecast(actual,predicted):
    scores=list()
    for i in range(actual.shape[1]):
      scores.append(sqrt(mean_squared_error(actual[:,i],predicted[:,i])))
    return scores

def new_eval_forecast(actual,predicted):
    score=0
  
    score=sqrt(mean_squared_error(actual[:],predicted[:]))
    return score

def reshape_samples_for_Conv1D(train_x):
    X1 = train_x[:,:,0].reshape(train_x.shape[0],train_x.shape[1],1)
    X2 = train_x[:,:,1].reshape(train_x.shape[0],train_x.shape[1],1)
    X3 = train_x[:,:,2].reshape(train_x.shape[0],train_x.shape[1],1)
    X4 = train_x[:,:,3].reshape(train_x.shape[0],train_x.shape[1],1)
    X5 = train_x[:,:,4].reshape(train_x.shape[0],train_x.shape[1],1)
    X6 = train_x[:,:,5].reshape(train_x.shape[0],train_x.shape[1],1)
    X7 = train_x[:,:,6].reshape(train_x.shape[0],train_x.shape[1],1)
    X8 = train_x[:,:,7].reshape(train_x.shape[0],train_x.shape[1],1)
    X9 = train_x[:,:,8].reshape(train_x.shape[0],train_x.shape[1],1)
    X10 = train_x[:,:,9].reshape(train_x.shape[0],train_x.shape[1],1)
    X11 = train_x[:,:,10].reshape(train_x.shape[0],train_x.shape[1],1)
    X12 = train_x[:,:,11].reshape(train_x.shape[0],train_x.shape[1],1)
    X13 = train_x[:,:,12].reshape(train_x.shape[0],train_x.shape[1],1)
    X14 = train_x[:,:,13].reshape(train_x.shape[0],train_x.shape[1],1)
    X15 = train_x[:,:,14].reshape(train_x.shape[0],train_x.shape[1],1)

    return X1,X2,X3,X4,X5,X6,X7,X8,X9,X10,X11,X12,X13,X14,X15

def CNN_GRU(n_timesteps):
    #15 submodels, a CNN is trained for each feature vector
    visible1 = Input(shape=(n_timesteps, 1))
    cnn1 = Conv1D(filters=64,kernel_size=2,strides=1,activation='relu')(visible1)
    #cnn1 = MaxPooling1D(pool_size=2)(cnn1)
    #cnn1 = Flatten(cnn1)

    visible2 = Input(shape=(n_timesteps, 1))
    cnn2 = Conv1D(filters=64,kernel_size=2,strides=1,activation='relu')(visible2)
    #cnn2 = MaxPooling1D(pool_size=2)(cnn2)
    #cnn2 = Flatten(cnn2)

    visible3 = Input(shape=(n_timesteps, 1))
    cnn3 = Conv1D(filters=64,kernel_size=2,strides=1,activation='relu')(visible3)
    #cnn3 = MaxPooling1D(pool_size=2)(cnn3)
    #cnn3 = Flatten(cnn3)
    
    visible4 = Input(shape=(n_timesteps, 1))
    cnn4 = Conv1D(filters=64,kernel_size=2,strides=1,activation='relu')(visible4)
    #cnn4 = MaxPooling1D(pool_size=2)(cnn4)
    #cnn4 = Flatten(cnn4)

    visible5 = Input(shape=(n_timesteps, 1))
    cnn5 = Conv1D(filters=64,kernel_size=2,strides=1,activation='relu')(visible5)
    #cnn5 = MaxPooling1D(pool_size=2)(cnn5)
    #cnn5 = Flatten(cnn5)

    visible6 = Input(shape=(n_timesteps, 1))
    cnn6 = Conv1D(filters=64,kernel_size=2,strides=1,activation='relu')(visible6)
    #cnn6 = MaxPooling1D(pool_size=2)(cnn6)
    #cnn6 = Flatten(cnn1)

    visible7 = Input(shape=(n_timesteps, 1))
    cnn7 = Conv1D(filters=64,kernel_size=2,strides=1,activation='relu')(visible7)
    #cnn7 = MaxPooling1D(pool_size=2)(cnn7)
    #cnn7 = Flatten(cnn7)

    visible8 = Input(shape=(n_timesteps, 1))
    cnn8 = Conv1D(filters=64,kernel_size=2,strides=1,activation='relu')(visible8)
    #cnn8 = MaxPooling1D(pool_size=2)(cnn8)
    #cnn8 = Flatten(cnn8)

    visible9 = Input(shape=(n_timesteps, 1))
    cnn9 = Conv1D(filters=64,kernel_size=2,strides=1,activation='relu')(visible9)
    #cnn9 = MaxPooling1D(pool_size=2)(cnn9)
    #cnn9 = Flatten(cnn9)

    visible10 = Input(shape=(n_timesteps, 1))
    cnn10 = Conv1D(filters=64,kernel_size=2,strides=1,activation='relu')(visible10)
    #cnn10 = MaxPooling1D(pool_size=2)(cnn10)
    #cnn10 = Flatten(cnn1)

    visible11 = Input(shape=(n_timesteps, 1))
    cnn11 = Conv1D(filters=64,kernel_size=2,strides=1,activation='relu')(visible11)
    #cnn11 = MaxPooling1D(pool_size=2)(cnn11)
    #cnn11 = Flatten(cnn11)

    visible12 = Input(shape=(n_timesteps, 1))
    cnn12 = Conv1D(filters=64,kernel_size=2,strides=1,activation='relu')(visible12)
    #cnn12 = MaxPooling1D(pool_size=2)(cnn12)
    #cnn12 = Flatten(cnn12)

    visible13 = Input(shape=(n_timesteps, 1))
    cnn13 = Conv1D(filters=64,kernel_size=2,strides=1,activation='relu')(visible13)
    #cnn13 = MaxPooling1D(pool_size=2)(cnn13)
    #cnn13 = Flatten(cnn13)

    visible14 = Input(shape=(n_timesteps, 1))
    cnn14 = Conv1D(filters=64,kernel_size=2,strides=1,activation='relu')(visible14)
    #cnn13 = MaxPooling1D(pool_size=2)(cnn13)
    #cnn13 = Flatten(cnn13)

    visible15 = Input(shape=(n_timesteps, 1))
    cnn15 = Conv1D(filters=64,kernel_size=2,strides=1,activation='relu')(visible15)

    merged = concatenate([cnn1,cnn2,cnn3,cnn4,cnn5,cnn6,cnn7,cnn8,cnn9,cnn10,cnn11,cnn12,cnn13,cnn14,cnn15]) # 3* (64*13)
    merged = GRU(50,activation='relu',return_sequences=True)(merged) # 3*50 # initially filters were 64, now 1
    merged = GRU(50,activation='relu',return_sequences=True)(merged)# 3*50
    merged = GRU(50,activation='relu',return_sequences=True)(merged)
    merged=  GRU(50,activation='relu')(merged) # 1*50
    merged = Dense(25)(merged)
    output = Dense(1)(merged) # 1*1

    model=Model(inputs=[visible1,visible2,visible3,visible4,visible5,visible6,visible7,visible8,visible9,visible10,visible11,visible12,visible13,visible14,visible15], outputs= output)
    model.compile(optimizer='adam',loss='mse')

    model.summary()
    return model

#CHANGE
#No need to reshape data1, it is already 2D. data1, that is, history ka shape is (1095*24, 14)
def new_forecast(model,history,n_input,is_conv):
    data1=np.array(history)
    data1.shape
    #data1=data1.reshape((data1.shape[0]*data1.shape[1],data1.shape[2]))
    ip_x=data1[-n_input:,0:]
    ip_x=ip_x.reshape(1,ip_x.shape[0],ip_x.shape[1])# 1*4*13
    # ADDITION TODAY

    if is_conv==1:
      X1 = ip_x[:,:,0].reshape(1,ip_x.shape[1],1)
      X2 = ip_x[:,:,1].reshape(1,ip_x.shape[1],1)
      X3 = ip_x[:,:,2].reshape(1,ip_x.shape[1],1)
      X4 = ip_x[:,:,3].reshape(1,ip_x.shape[1],1)
      X5 = ip_x[:,:,4].reshape(1,ip_x.shape[1],1)
      X6 = ip_x[:,:,5].reshape(1,ip_x.shape[1],1)
      X7 = ip_x[:,:,6].reshape(1,ip_x.shape[1],1)
      X8 = ip_x[:,:,7].reshape(1,ip_x.shape[1],1)
      X9 = ip_x[:,:,8].reshape(1,ip_x.shape[1],1)
      X10 = ip_x[:,:,9].reshape(1,ip_x.shape[1],1)
      X11= ip_x[:,:,10].reshape(1,ip_x.shape[1],1)
      X12 = ip_x[:,:,11].reshape(1,ip_x.shape[1],1)
      X13 = ip_x[:,:,12].reshape(1,ip_x.shape[1],1)
      X14 = ip_x[:,:,13].reshape(1,ip_x.shape[1],1)
      X15 = ip_x[:,:,14].reshape(1,ip_x.shape[1],1)

      y_pred=model.predict([X1,X2,X3,X4,X5,X6,X7,X8,X9,X10,X11,X12,X13,X14,X15],verbose=0)
    
    else:
      y_pred = model.predict(ip_x,verbose=0)


    y_pred=y_pred[0]
    return y_pred

def fit_model(train_x,train_y,model,model_number,istl):
  #how to get train_x, train_y from model arch return function
  history = History()
  if istl==0:
    filepath=os.path.join('best_weights'+ str(model_number) + '.hdf5')
  elif istl==1:
    filepath=os.path.join('best_weights'+ str(model_number) + 'withoutTL.hdf5')
  elif istl==2:
    filepath=os.path.join('best_weights'+ str(model_number) + 'withTL.hdf5')
    
  checkpoint= ModelCheckpoint(filepath,monitor='val_loss',save_best_only=True,mode='min')
  callbacks_list = [checkpoint]

  if model_number<3:
     X1,X2,X3,X4,X5,X6,X7,X8,X9,X10,X11,X12,X13,X14,X15 = reshape_samples_for_Conv1D(train_x)
     history= model.fit([X1,X2,X3,X4,X5,X6,X7,X8,X9,X10,X11,X12,X13,X14,X15],train_y,epochs=5,batch_size=15,verbose=1,callbacks=callbacks_list,validation_split= 0.1)

  else:
     history= model.fit(train_x,train_y,epochs=20,batch_size=15,verbose=1,callbacks=callbacks_list,validation_split= 0.3)
  return history

def to_supervised(train,n_input,n_out):
    data=train.reshape((train.shape[0]*train.shape[1],train.shape[2]))
    x,y=list(),list()
    start=0
    for i in range(len(data)):
        in_end=start+n_input
        out_end=in_end+n_out
        if out_end>len(data):
            break
        x.append(data[start:in_end,0:])
        y.append(data[in_end:out_end,0])
        start=start+1
    return np.array(x),np.array(y)

def build_model(train,n_input):
    train_x,train_y=to_supervised(train,n_input,1)
    print("labels created")
    print(train_x.shape[0])
    n_timesteps,n_features,n_outputs=train_x.shape[1],train_x.shape[2],train_y.shape[1]
    print(train_x.shape)
    print(train_y.shape)
    model=Sequential()
    model.add(LSTM(32,activation='relu',return_sequences=True, input_shape=(n_timesteps,n_features)))
    model.add(LSTM(32,activation='relu',return_sequences=True))
    model.add(LSTM(32,activation='relu',return_sequences=True))
    model.add(LSTM(32,activation='relu'))
    
    model.add(Dense(100,activation='relu'))
    model.add(Dense(1))
    model.compile(loss='mse',optimizer='adam')
    
    model.summary()
    model.fit(train_x,train_y,epochs=10,batch_size=50,verbose=1, validation_split=0.3)
    return model

def buildmodelarc(n_timesteps,n_features):
    model=Sequential()
    model.add(LSTM(32,activation='relu',return_sequences=True,input_shape=(n_timesteps,n_features)))
    model.add(LSTM(32,activation='relu',return_sequences=True))
    model.add(LSTM(32,activation='relu',return_sequences=True))
    model.add(LSTM(32,activation='relu'))
    
    model.add(Dense(100,activation='relu'))
    model.add(Dense(1))
    model.compile(loss='mse',optimizer='adam')
    
    model.summary()
    return model

def forecast(model,history,n_input):
    data=np.array(history)
    ip_x=data[-n_input:,0:]
    ip_x=ip_x.reshape(1,ip_x.shape[0],ip_x.shape[1])
    y_pred=model.predict(ip_x,verbose=0)
    y_pred=y_pred[0]
    return y_pred

def model_eval(train,test,n_input,model,split_val,i):
    if i==0:
      model.load_weights("best_weights0.hdf5")
    print("model built")
    new_train=train.reshape((train.shape[0]*train.shape[1],train.shape[2]))
    new_test=test.reshape((test.shape[0]*test.shape[1],test.shape[2]))
    his=[x for x in new_train]
    
    predictions=list()
    for i in range(len(new_test)):
        predictions.append(new_forecast(model,his,n_input,1))
        his.append(new_test[i,])
    predictions=np.array(predictions)
    print(predictions.shape)

    new_predictions=np.array(np.split(predictions,len(predictions)/split_val))
    pyplot.figure(figsize=(15,8))
    pyplot.plot(new_test[:,0], linewidth='3')
    pyplot.plot(predictions[:,0], color='red', linewidth='2')
    pyplot.show()
    scores=eval_forecast(test[:,:,0],new_predictions[:,:,0])
    final_score=new_eval_forecast(new_test[:,0],predictions[:,0])

    return scores,final_score

def ps(scores,ts,min,max):
  avgrmse=0
  scores=[t*(max-min)+min for t in scores]
  for i in range(len(scores)):
    avgrmse=avgrmse+scores[i]
  avgrmse=avgrmse/ts
  
  print(scores)
  print("Average Rmse: ",end=' ')
  print(avgrmse)

def main():

  data_fp=[]
  data_fp.insert(0,'/content/Aotizhongxinpreproc.csv')
  data_fp.insert(1,'/content/Dinglingpreproc.csv')
  data_fp.insert(2,'/content/Dongsipreproc.csv')
  data_fp.insert(3,'/content/Guanyuanpreproc.csv')
  data_fp.insert(4,'/content/Guchengpreproc.csv')
  data_fp.insert(5,'/content/Huairoupreproc.csv')
  data_fp.insert(6,'/content/Nongzhanguanpreproc.csv')
  data_fp.insert(7,'/content/Shunyipreproc.csv')
  data_fp.insert(8,'/content/Tiantanpreproc.csv')
  data_fp.insert(9,'/content/Wanliupreproc.csv')
  data_fp.insert(10,'/content/Wanshouxigongpreproc.csv')

  i=0
  model=model=CNN_GRU(24)
  print(len(model.layers))
  for city in data_fp:
    print(city)
    data=pd.read_csv(city,header=0,parse_dates=True,index_col=['No'])
    n_input=4
    min=data['PM2.5'].min()
    max=data['PM2.5'].max()
    scaler = MinMaxScaler(feature_range=(0, 1))
    data = scaler.fit_transform(data)
    print(min)
    print(max)

    if i==0:
      train,test=split(data)
    else:
      train,test=new_split(data)

      for j in range(30,34):
        model.get_layer(index=i).trainable=False
      model.compile(optimizer='adam',loss='mse')
    
    print(train.shape)
    print(test.shape)

    train_x,train_y=to_supervised(train,n_input,1)
    n_timesteps,n_features,n_outputs= train_x.shape[1],train_x.shape[2],train_y.shape[1]
    train_y = train_y.reshape((train_y.shape[0],train_y.shape[1]))
    print(train_x.shape)
    print(train_y.shape)

    #basemodel=build_model(train,4)
    #scores,final_score=model_eval(train,test,4,basemodel,24)
    #scores=[t*(max-min)+min for t in scores]
    #print(scores)
    #print(final_score*(max-min)+min)
    if i==0:
      model=CNN_GRU(n_timesteps)

    history = fit_model(train_x,train_y,model,0,0)
    scores,final_score= model_eval(train,test,n_input,model,24,0)
    scores=[t*(max-min)+min for t in scores]
    print(scores)
    print(final_score*(max-min)+min)

    filename = 'fm'+str(i)+'.sav'
    pickle.dump(model, open(filename, 'wb'))
    i=i+1

if __name__ == '__main__':
  main()
